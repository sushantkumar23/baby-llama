{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.q = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.k = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.v = nn.Linear(embed_dim, hidden_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        d_k = q.size(-1)\n",
    "        attn_logits = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, -torch.inf)\n",
    "        weights = F.softmax(attn_logits, dim=-1)\n",
    "        return torch.bmm(scores, v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = self.q(x), self.k(x), self.v(x)\n",
    "        return self.scaled_dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_dim: 768, n_heads: 6, head_dim: 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing with random inputs\n",
    "hidden_dim = 768\n",
    "seq_len = 10\n",
    "batch_size = 32\n",
    "n_heads = 6\n",
    "head_dim = hidden_dim // n_heads\n",
    "print(f'hidden_dim: {hidden_dim}, n_heads: {n_heads}, head_dim: {head_dim}')\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1354, -0.0800, -0.2062,  ...,  0.0406, -0.0935, -0.0527],\n",
       "         [ 0.2048, -0.0702, -0.2163,  ..., -0.0254, -0.2176,  0.1759],\n",
       "         [ 0.0806, -0.1309, -0.1470,  ...,  0.0566, -0.0871, -0.0675],\n",
       "         ...,\n",
       "         [ 0.0487, -0.0901, -0.1056,  ...,  0.1106, -0.1494,  0.0155],\n",
       "         [ 0.1283, -0.0952, -0.1791,  ...,  0.0519, -0.1772,  0.1507],\n",
       "         [ 0.2081, -0.0737, -0.2328,  ...,  0.0425, -0.1503,  0.0442]],\n",
       "\n",
       "        [[ 0.2795, -0.0317,  0.0665,  ...,  0.1240, -0.0638, -0.1576],\n",
       "         [ 0.3139, -0.0117,  0.1160,  ...,  0.1114,  0.0058, -0.2802],\n",
       "         [ 0.2876,  0.0218,  0.0989,  ..., -0.0214,  0.0939, -0.1724],\n",
       "         ...,\n",
       "         [ 0.3883, -0.0011,  0.0167,  ...,  0.0582,  0.0843, -0.2127],\n",
       "         [ 0.4049, -0.0512,  0.0827,  ...,  0.1879, -0.0714, -0.3038],\n",
       "         [ 0.4627, -0.0953,  0.0712,  ..., -0.0777,  0.1226, -0.2759]],\n",
       "\n",
       "        [[ 0.0140, -0.0202,  0.1011,  ...,  0.0093,  0.0582,  0.0336],\n",
       "         [ 0.1245, -0.1210, -0.0290,  ..., -0.0880,  0.0414, -0.0373],\n",
       "         [ 0.0120, -0.0729,  0.0987,  ..., -0.0870, -0.0256, -0.0986],\n",
       "         ...,\n",
       "         [ 0.1021, -0.1022, -0.0330,  ..., -0.0497,  0.0506,  0.0557],\n",
       "         [ 0.0605, -0.0919, -0.0600,  ..., -0.0488,  0.0522, -0.0055],\n",
       "         [ 0.1548,  0.0116, -0.0543,  ...,  0.0601,  0.1132,  0.0649]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3616,  0.2592,  0.0189,  ..., -0.1292,  0.0132,  0.1212],\n",
       "         [ 0.3548,  0.2237,  0.0509,  ..., -0.1184,  0.1198,  0.1162],\n",
       "         [ 0.3959,  0.2939,  0.1013,  ..., -0.0318,  0.1367,  0.0443],\n",
       "         ...,\n",
       "         [ 0.3334,  0.1889,  0.0643,  ..., -0.0960,  0.1646,  0.1043],\n",
       "         [ 0.4323,  0.3022,  0.0973,  ..., -0.1128,  0.0401,  0.0978],\n",
       "         [ 0.4358,  0.2777,  0.1325,  ..., -0.0598,  0.0749,  0.0705]],\n",
       "\n",
       "        [[ 0.0473, -0.0066,  0.2818,  ...,  0.1401,  0.4825,  0.0129],\n",
       "         [ 0.0094,  0.0620,  0.3624,  ...,  0.0234,  0.2721, -0.1086],\n",
       "         [ 0.0040,  0.0425,  0.3216,  ...,  0.0655,  0.2280, -0.1595],\n",
       "         ...,\n",
       "         [ 0.1002,  0.0906,  0.3070,  ...,  0.1954,  0.3047, -0.0577],\n",
       "         [ 0.0364,  0.0241,  0.2993,  ...,  0.1980,  0.3109,  0.0041],\n",
       "         [ 0.1361, -0.0082,  0.0933,  ...,  0.1963,  0.3298, -0.0060]],\n",
       "\n",
       "        [[-0.3115, -0.0773, -0.3266,  ...,  0.0928,  0.0647,  0.1308],\n",
       "         [-0.2215, -0.0457, -0.2852,  ...,  0.2268,  0.0585,  0.1368],\n",
       "         [-0.2566, -0.0580, -0.1977,  ...,  0.2487,  0.0956,  0.0952],\n",
       "         ...,\n",
       "         [-0.3293, -0.0800, -0.2373,  ...,  0.2129,  0.0091,  0.0529],\n",
       "         [-0.2048, -0.0110, -0.3451,  ...,  0.1983,  0.1295,  0.1421],\n",
       "         [-0.3494, -0.0890, -0.2295,  ...,  0.2810,  0.0080, -0.0480]]],\n",
       "       grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttention(hidden_dim, head_dim)\n",
    "output = sa(x)\n",
    "print(output.shape)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // n_heads\n",
    "        assert self.head_dim * n_heads == hidden_dim, \"Hidden dimension must be divisible \"\n",
    "        \"by the number of heads\"\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttention(self.hidden_dim, self.head_dim) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return self.out_proj(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "n_heads = 8\n",
    "\n",
    "mha = MultiHeadAttention(n_heads=n_heads, hidden_dim=hidden_dim)\n",
    "output = mha(x)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, intermediate_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.out = nn.Linear(intermediate_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.silu(self.fc1(x)) + self.fc2(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, hidden_dim, intermediate_dim):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, hidden_dim)\n",
    "        self.mlp = MLP(hidden_dim, intermediate_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.mha(x) + x\n",
    "        out = self.mlp(h) + h\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyLlama(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, max_seq_len, n_layers, n_heads, hidden_dim, intermediate_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, hidden_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            Block(n_heads, hidden_dim, intermediate_dim) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_embeddings = self.token_embedding(x)\n",
    "\n",
    "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n",
    "        pos_embeddings = self.pos_embedding(positions)\n",
    "\n",
    "        x = tok_embeddings + pos_embeddings\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        logits = self.out(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 32_000\n",
    "max_seq_len = 1024\n",
    "n_layers = 6\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
